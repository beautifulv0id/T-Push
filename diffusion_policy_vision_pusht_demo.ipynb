{"cells":[{"cell_type":"markdown","metadata":{},"source":[" ### **Imports**"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["2ed040bf97024baca8cefaa0e35bed9e","0b0ca1b2ad5842128c4a38b5d4439298","915df5cf55204847bb3ebe17417dd200","795c32269ac54be5b832a58f21d7c8d3","18d2e12e4b224a22b1d1aa76c98a3ae4","56391f008fe74cdaa1ba1748154abf8f","5fa086a4e27b40c39f56bb259bdca964","119814c5a6904193a5cca0a4fd67a47e","95b06f2fff024d3c8a3a447402074922","c8c100153bd94663bfc83071c81eb7a4","22c2c5ca95b94e1daa707147d098f8ca"]},"executionInfo":{"elapsed":15331,"status":"ok","timestamp":1702476299423,"user":{"displayName":"Felix Herrmann","userId":"07905804872157634150"},"user_tz":-60},"id":"VrX4VTl5pYNq","outputId":"b4698639-9bd8-4b27-f20c-c8c3b2ec759c"},"outputs":[],"source":["# diffusion policy import\n","from typing import Tuple, Sequence, Dict, Union, Optional, Callable\n","import numpy as np\n","import math\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import collections\n","import zarr\n","from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n","from diffusers.training_utils import EMAModel\n","from diffusers.optimization import get_scheduler\n","from tqdm.auto import tqdm\n","\n","# env import\n","import gym\n","from gym import spaces\n","import pygame\n","import pymunk\n","import pymunk.pygame_util\n","from pymunk.space_debug_draw_options import SpaceDebugColor\n","from pymunk.vec2d import Vec2d\n","import shapely.geometry as sg\n","import cv2\n","import skimage.transform as st\n","from skvideo.io import vwrite\n","from IPython.display import Video\n","import gdown\n","import os\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["### **Environment**\n","Defines a PyMunk-based Push-T environment `PushTEnv`.\n","And it's subclass `PushTImageEnv`.\n","\n","**Goal**: push the gray T-block into the green area.\n","\n","Adapted from [Implicit Behavior Cloning](https://implicitbc.github.io/)"]},{"cell_type":"code","execution_count":95,"metadata":{"cellView":"form","executionInfo":{"elapsed":431,"status":"ok","timestamp":1702476299849,"user":{"displayName":"Felix Herrmann","userId":"07905804872157634150"},"user_tz":-60},"id":"L5E-nR6ornyg"},"outputs":[],"source":["positive_y_is_up: bool = False\n","\"\"\"Make increasing values of y point upwards.\n","\n","When True::\n","\n","    y\n","    ^\n","    |      . (3, 3)\n","    |\n","    |   . (2, 2)\n","    |\n","    +------ > x\n","\n","When False::\n","\n","    +------ > x\n","    |\n","    |   . (2, 2)\n","    |\n","    |      . (3, 3)\n","    v\n","    y\n","\n","\"\"\"\n","\n","def to_pygame(p: Tuple[float, float], surface: pygame.Surface) -> Tuple[int, int]:\n","    \"\"\"Convenience method to convert pymunk coordinates to pygame surface\n","    local coordinates.\n","\n","    Note that in case positive_y_is_up is False, this function wont actually do\n","    anything except converting the point to integers.\n","    \"\"\"\n","    if positive_y_is_up:\n","        return round(p[0]), surface.get_height() - round(p[1])\n","    else:\n","        return round(p[0]), round(p[1])\n","\n","\n","def light_color(color: SpaceDebugColor):\n","    color = np.minimum(1.2 * np.float32([color.r, color.g, color.b, color.a]), np.float32([255]))\n","    color = SpaceDebugColor(r=color[0], g=color[1], b=color[2], a=color[3])\n","    return color\n","\n","class DrawOptions(pymunk.SpaceDebugDrawOptions):\n","    def __init__(self, surface: pygame.Surface) -> None:\n","        \"\"\"Draw a pymunk.Space on a pygame.Surface object.\n","\n","        Typical usage::\n","\n","        >>> import pymunk\n","        >>> surface = pygame.Surface((10,10))\n","        >>> space = pymunk.Space()\n","        >>> options = pymunk.pygame_util.DrawOptions(surface)\n","        >>> space.debug_draw(options)\n","\n","        You can control the color of a shape by setting shape.color to the color\n","        you want it drawn in::\n","\n","        >>> c = pymunk.Circle(None, 10)\n","        >>> c.color = pygame.Color(\"pink\")\n","\n","        See pygame_util.demo.py for a full example\n","\n","        Since pygame uses a coordiante system where y points down (in contrast\n","        to many other cases), you either have to make the physics simulation\n","        with Pymunk also behave in that way, or flip everything when you draw.\n","\n","        The easiest is probably to just make the simulation behave the same\n","        way as Pygame does. In that way all coordinates used are in the same\n","        orientation and easy to reason about::\n","\n","        >>> space = pymunk.Space()\n","        >>> space.gravity = (0, -1000)\n","        >>> body = pymunk.Body()\n","        >>> body.position = (0, 0) # will be positioned in the top left corner\n","        >>> space.debug_draw(options)\n","\n","        To flip the drawing its possible to set the module property\n","        :py:data:`positive_y_is_up` to True. Then the pygame drawing will flip\n","        the simulation upside down before drawing::\n","\n","        >>> positive_y_is_up = True\n","        >>> body = pymunk.Body()\n","        >>> body.position = (0, 0)\n","        >>> # Body will be position in bottom left corner\n","\n","        :Parameters:\n","                surface : pygame.Surface\n","                    Surface that the objects will be drawn on\n","        \"\"\"\n","        self.surface = surface\n","        super(DrawOptions, self).__init__()\n","\n","    def draw_circle(\n","        self,\n","        pos: Vec2d,\n","        angle: float,\n","        radius: float,\n","        outline_color: SpaceDebugColor,\n","        fill_color: SpaceDebugColor,\n","    ) -> None:\n","        p = to_pygame(pos, self.surface)\n","\n","        pygame.draw.circle(self.surface, fill_color.as_int(), p, round(radius), 0)\n","        pygame.draw.circle(self.surface, light_color(fill_color).as_int(), p, round(radius-4), 0)\n","\n","        circle_edge = pos + Vec2d(radius, 0).rotated(angle)\n","        p2 = to_pygame(circle_edge, self.surface)\n","        line_r = 2 if radius > 20 else 1\n","        # pygame.draw.lines(self.surface, outline_color.as_int(), False, [p, p2], line_r)\n","\n","    def draw_segment(self, a: Vec2d, b: Vec2d, color: SpaceDebugColor) -> None:\n","        p1 = to_pygame(a, self.surface)\n","        p2 = to_pygame(b, self.surface)\n","\n","        pygame.draw.aalines(self.surface, color.as_int(), False, [p1, p2])\n","\n","    def draw_fat_segment(\n","        self,\n","        a: Tuple[float, float],\n","        b: Tuple[float, float],\n","        radius: float,\n","        outline_color: SpaceDebugColor,\n","        fill_color: SpaceDebugColor,\n","    ) -> None:\n","        p1 = to_pygame(a, self.surface)\n","        p2 = to_pygame(b, self.surface)\n","\n","        r = round(max(1, radius * 2))\n","        pygame.draw.lines(self.surface, fill_color.as_int(), False, [p1, p2], r)\n","        if r > 2:\n","            orthog = [abs(p2[1] - p1[1]), abs(p2[0] - p1[0])]\n","            if orthog[0] == 0 and orthog[1] == 0:\n","                return\n","            scale = radius / (orthog[0] * orthog[0] + orthog[1] * orthog[1]) ** 0.5\n","            orthog[0] = round(orthog[0] * scale)\n","            orthog[1] = round(orthog[1] * scale)\n","            points = [\n","                (p1[0] - orthog[0], p1[1] - orthog[1]),\n","                (p1[0] + orthog[0], p1[1] + orthog[1]),\n","                (p2[0] + orthog[0], p2[1] + orthog[1]),\n","                (p2[0] - orthog[0], p2[1] - orthog[1]),\n","            ]\n","            pygame.draw.polygon(self.surface, fill_color.as_int(), points)\n","            pygame.draw.circle(\n","                self.surface,\n","                fill_color.as_int(),\n","                (round(p1[0]), round(p1[1])),\n","                round(radius),\n","            )\n","            pygame.draw.circle(\n","                self.surface,\n","                fill_color.as_int(),\n","                (round(p2[0]), round(p2[1])),\n","                round(radius),\n","            )\n","\n","    def draw_polygon(\n","        self,\n","        verts: Sequence[Tuple[float, float]],\n","        radius: float,\n","        outline_color: SpaceDebugColor,\n","        fill_color: SpaceDebugColor,\n","    ) -> None:\n","        ps = [to_pygame(v, self.surface) for v in verts]\n","        ps += [ps[0]]\n","\n","        radius = 2\n","        pygame.draw.polygon(self.surface, light_color(fill_color).as_int(), ps)\n","\n","        if radius > 0:\n","            for i in range(len(verts)):\n","                a = verts[i]\n","                b = verts[(i + 1) % len(verts)]\n","                self.draw_fat_segment(a, b, radius, fill_color, fill_color)\n","\n","    def draw_dot(\n","        self, size: float, pos: Tuple[float, float], color: SpaceDebugColor\n","    ) -> None:\n","        p = to_pygame(pos, self.surface)\n","        pygame.draw.circle(self.surface, color.as_int(), p, round(size), 0)\n","\n","\n","def pymunk_to_shapely(body, shapes):\n","    geoms = list()\n","    for shape in shapes:\n","        if isinstance(shape, pymunk.shapes.Poly):\n","            verts = [body.local_to_world(v) for v in shape.get_vertices()]\n","            verts += [verts[0]]\n","            geoms.append(sg.Polygon(verts))\n","        else:\n","            raise RuntimeError(f'Unsupported shape type {type(shape)}')\n","    geom = sg.MultiPolygon(geoms)\n","    return geom\n","\n","# env\n","class PushTEnv(gym.Env):\n","    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 10}\n","    reward_range = (0., 1.)\n","\n","    def __init__(self,\n","            legacy=False,\n","            block_cog=None, damping=None,\n","            render_action=True,\n","            render_size=96,\n","            reset_to_state=None\n","        ):\n","        self._seed = None\n","        self.seed()\n","        self.window_size = ws = 512  # The size of the PyGame window\n","        self.render_size = render_size\n","        self.sim_hz = 100\n","        # Local controller params.\n","        self.k_p, self.k_v = 100, 20    # PD control.z\n","        self.control_hz = self.metadata['video.frames_per_second']\n","        # legcay set_state for data compatiblity\n","        self.legacy = legacy\n","\n","        # agent_pos, block_pos, block_angle\n","        self.observation_space = spaces.Box(\n","            low=np.array([0,0,0,0,0], dtype=np.float64),\n","            high=np.array([ws,ws,ws,ws,np.pi*2], dtype=np.float64),\n","            shape=(5,),\n","            dtype=np.float64\n","        )\n","\n","        # positional goal for agent\n","        self.action_space = spaces.Box(\n","            low=np.array([0,0], dtype=np.float64),\n","            high=np.array([ws,ws], dtype=np.float64),\n","            shape=(2,),\n","            dtype=np.float64\n","        )\n","\n","        self.block_cog = block_cog\n","        self.damping = damping\n","        self.render_action = render_action\n","\n","        \"\"\"\n","        If human-rendering is used, `self.window` will be a reference\n","        to the window that we draw to. `self.clock` will be a clock that is used\n","        to ensure that the environment is rendered at the correct framerate in\n","        human-mode. They will remain `None` until human-mode is used for the\n","        first time.\n","        \"\"\"\n","        self.window = None\n","        self.clock = None\n","        self.screen = None\n","\n","        self.space = None\n","        self.teleop = None\n","        self.render_buffer = None\n","        self.latest_action = None\n","        self.reset_to_state = reset_to_state\n","\n","    def reset(self):\n","        seed = self._seed\n","        self._setup()\n","        if self.block_cog is not None:\n","            self.block.center_of_gravity = self.block_cog\n","        if self.damping is not None:\n","            self.space.damping = self.damping\n","\n","        # use legacy RandomState for compatiblity\n","        state = self.reset_to_state\n","        if state is None:\n","            rs = np.random.RandomState(seed=seed)\n","            state = np.array([\n","                rs.randint(50, 450), rs.randint(50, 450),\n","                rs.randint(100, 400), rs.randint(100, 400),\n","                rs.randn() * 2 * np.pi - np.pi\n","                ])\n","        self._set_state(state)\n","\n","        obs = self._get_obs()\n","        info = self._get_info()\n","        return obs, info\n","\n","    def step(self, action):\n","        dt = 1.0 / self.sim_hz\n","        self.n_contact_points = 0\n","        n_steps = self.sim_hz // self.control_hz\n","        if action is not None:\n","            self.latest_action = action\n","            for i in range(n_steps):\n","                # Step PD control.\n","                # self.agent.velocity = self.k_p * (act - self.agent.position)    # P control works too.\n","                acceleration = self.k_p * (action - self.agent.position) + self.k_v * (Vec2d(0, 0) - self.agent.velocity)\n","                self.agent.velocity += acceleration * dt\n","\n","                # Step physics.\n","                self.space.step(dt)\n","\n","        # compute reward\n","        goal_body = self._get_goal_pose_body(self.goal_pose)\n","        goal_geom = pymunk_to_shapely(goal_body, self.block.shapes)\n","        block_geom = pymunk_to_shapely(self.block, self.block.shapes)\n","\n","        intersection_area = goal_geom.intersection(block_geom).area\n","        goal_area = goal_geom.area\n","        coverage = intersection_area / goal_area\n","        reward = np.clip(coverage / self.success_threshold, 0, 1)\n","        done = coverage > self.success_threshold\n","        terminated = done\n","        truncated = done\n","\n","        observation = self._get_obs()\n","        info = self._get_info()\n","\n","        return observation, reward, terminated, truncated, info\n","\n","    def render(self, mode):\n","        return self._render_frame(mode)\n","\n","    def teleop_agent(self):\n","        TeleopAgent = collections.namedtuple('TeleopAgent', ['act'])\n","        def act(obs):\n","            act = None\n","            mouse_position = pymunk.pygame_util.from_pygame(Vec2d(*pygame.mouse.get_pos()), self.screen)\n","            if self.teleop or (mouse_position - self.agent.position).length < 30:\n","                self.teleop = True\n","                act = mouse_position\n","            return act\n","        return TeleopAgent(act)\n","\n","    def _get_obs(self):\n","        obs = np.array(\n","            tuple(self.agent.position) \\\n","            + tuple(self.block.position) \\\n","            + (self.block.angle % (2 * np.pi),))\n","        return obs\n","\n","    def _get_goal_pose_body(self, pose):\n","        mass = 1\n","        inertia = pymunk.moment_for_box(mass, (50, 100))\n","        body = pymunk.Body(mass, inertia)\n","        # preserving the legacy assignment order for compatibility\n","        # the order here dosn't matter somehow, maybe because CoM is aligned with body origin\n","        body.position = pose[:2].tolist()\n","        body.angle = pose[2]\n","        return body\n","\n","    def _get_info(self):\n","        n_steps = self.sim_hz // self.control_hz\n","        n_contact_points_per_step = int(np.ceil(self.n_contact_points / n_steps))\n","        info = {\n","            'pos_agent': np.array(self.agent.position),\n","            'vel_agent': np.array(self.agent.velocity),\n","            'block_pose': np.array(list(self.block.position) + [self.block.angle]),\n","            'goal_pose': self.goal_pose,\n","            'n_contacts': n_contact_points_per_step}\n","        return info\n","\n","    def _render_frame(self, mode):\n","\n","        if self.window is None and mode == \"human\":\n","            pygame.init()\n","            pygame.display.init()\n","            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n","        if self.clock is None and mode == \"human\":\n","            self.clock = pygame.time.Clock()\n","\n","        canvas = pygame.Surface((self.window_size, self.window_size))\n","        canvas.fill((255, 255, 255))\n","        self.screen = canvas\n","\n","        draw_options = DrawOptions(canvas)\n","\n","        # Draw goal pose.\n","        goal_body = self._get_goal_pose_body(self.goal_pose)\n","        for shape in self.block.shapes:\n","            goal_points = [pymunk.pygame_util.to_pygame(goal_body.local_to_world(v), draw_options.surface) for v in shape.get_vertices()]\n","            goal_points += [goal_points[0]]\n","            pygame.draw.polygon(canvas, self.goal_color, goal_points)\n","\n","        # Draw agent and block.\n","        self.space.debug_draw(draw_options)\n","\n","        if mode == \"human\":\n","            # The following line copies our drawings from `canvas` to the visible window\n","            self.window.blit(canvas, canvas.get_rect())\n","            pygame.event.pump()\n","            pygame.display.update()\n","\n","            # the clock is aleady ticked during in step for \"human\"\n","\n","\n","        img = np.transpose(\n","                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n","            )\n","        img = cv2.resize(img, (self.render_size, self.render_size))\n","        if self.render_action:\n","            if self.render_action and (self.latest_action is not None):\n","                action = np.array(self.latest_action)\n","                coord = (action / 512 * 96).astype(np.int32)\n","                marker_size = int(8/96*self.render_size)\n","                thickness = int(1/96*self.render_size)\n","                cv2.drawMarker(img, coord,\n","                    color=(255,0,0), markerType=cv2.MARKER_CROSS,\n","                    markerSize=marker_size, thickness=thickness)\n","        return img\n","\n","\n","    def close(self):\n","        if self.window is not None:\n","            pygame.display.quit()\n","            pygame.quit()\n","\n","    def seed(self, seed=None):\n","        if seed is None:\n","            seed = np.random.randint(0,25536)\n","        self._seed = seed\n","        self.np_random = np.random.default_rng(seed)\n","\n","    def _handle_collision(self, arbiter, space, data):\n","        self.n_contact_points += len(arbiter.contact_point_set.points)\n","\n","    def _set_state(self, state):\n","        if isinstance(state, np.ndarray):\n","            state = state.tolist()\n","        pos_agent = state[:2]\n","        pos_block = state[2:4]\n","        rot_block = state[4]\n","        self.agent.position = pos_agent\n","        # setting angle rotates with respect to center of mass\n","        # therefore will modify the geometric position\n","        # if not the same as CoM\n","        # therefore should be modified first.\n","        if self.legacy:\n","            # for compatiblity with legacy data\n","            self.block.position = pos_block\n","            self.block.angle = rot_block\n","        else:\n","            self.block.angle = rot_block\n","            self.block.position = pos_block\n","\n","        # Run physics to take effect\n","        self.space.step(1.0 / self.sim_hz)\n","\n","    def _set_state_local(self, state_local):\n","        agent_pos_local = state_local[:2]\n","        block_pose_local = state_local[2:]\n","        tf_img_obj = st.AffineTransform(\n","            translation=self.goal_pose[:2],\n","            rotation=self.goal_pose[2])\n","        tf_obj_new = st.AffineTransform(\n","            translation=block_pose_local[:2],\n","            rotation=block_pose_local[2]\n","        )\n","        tf_img_new = st.AffineTransform(\n","            matrix=tf_img_obj.params @ tf_obj_new.params\n","        )\n","        agent_pos_new = tf_img_new(agent_pos_local)\n","        new_state = np.array(\n","            list(agent_pos_new[0]) + list(tf_img_new.translation) \\\n","                + [tf_img_new.rotation])\n","        self._set_state(new_state)\n","        return new_state\n","\n","    def _setup(self):\n","        self.space = pymunk.Space()\n","        self.space.gravity = 0, 0\n","        self.space.damping = 0\n","        self.teleop = False\n","        self.render_buffer = list()\n","\n","        # Add walls.\n","        walls = [\n","            self._add_segment((5, 506), (5, 5), 2),\n","            self._add_segment((5, 5), (506, 5), 2),\n","            self._add_segment((506, 5), (506, 506), 2),\n","            self._add_segment((5, 506), (506, 506), 2)\n","        ]\n","        self.space.add(*walls)\n","\n","        # Add agent, block, and goal zone.\n","        self.agent = self.add_circle((256, 400), 15)\n","        self.block = self.add_tee((256, 300), 0)\n","        self.goal_color = pygame.Color('LightGreen')\n","        self.goal_pose = np.array([256,256,np.pi/4])  # x, y, theta (in radians)\n","\n","        # Add collision handeling\n","        self.collision_handeler = self.space.add_collision_handler(0, 0)\n","        self.collision_handeler.post_solve = self._handle_collision\n","        self.n_contact_points = 0\n","\n","        self.max_score = 50 * 100\n","        self.success_threshold = 0.95    # 95% coverage.\n","\n","    def _add_segment(self, a, b, radius):\n","        shape = pymunk.Segment(self.space.static_body, a, b, radius)\n","        shape.color = pygame.Color('LightGray')    # https://htmlcolorcodes.com/color-names\n","        return shape\n","\n","    def add_circle(self, position, radius):\n","        body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)\n","        body.position = position\n","        body.friction = 1\n","        shape = pymunk.Circle(body, radius)\n","        shape.color = pygame.Color('RoyalBlue')\n","        self.space.add(body, shape)\n","        return body\n","\n","    def add_box(self, position, height, width):\n","        mass = 1\n","        inertia = pymunk.moment_for_box(mass, (height, width))\n","        body = pymunk.Body(mass, inertia)\n","        body.position = position\n","        shape = pymunk.Poly.create_box(body, (height, width))\n","        shape.color = pygame.Color('LightSlateGray')\n","        self.space.add(body, shape)\n","        return body\n","\n","    def add_tee(self, position, angle, scale=30, color='LightSlateGray', mask=pymunk.ShapeFilter.ALL_MASKS()):\n","        mass = 1\n","        length = 4\n","        vertices1 = [(-length*scale/2, scale),\n","                                 ( length*scale/2, scale),\n","                                 ( length*scale/2, 0),\n","                                 (-length*scale/2, 0)]\n","        inertia1 = pymunk.moment_for_poly(mass, vertices=vertices1)\n","        vertices2 = [(-scale/2, scale),\n","                                 (-scale/2, length*scale),\n","                                 ( scale/2, length*scale),\n","                                 ( scale/2, scale)]\n","        inertia2 = pymunk.moment_for_poly(mass, vertices=vertices1)\n","        body = pymunk.Body(mass, inertia1 + inertia2)\n","        shape1 = pymunk.Poly(body, vertices1)\n","        shape2 = pymunk.Poly(body, vertices2)\n","        shape1.color = pygame.Color(color)\n","        shape2.color = pygame.Color(color)\n","        shape1.filter = pymunk.ShapeFilter(mask=mask)\n","        shape2.filter = pymunk.ShapeFilter(mask=mask)\n","        body.center_of_gravity = (shape1.center_of_gravity + shape2.center_of_gravity) / 2\n","        body.position = position\n","        body.angle = angle\n","        body.friction = 1\n","        self.space.add(body, shape1, shape2)\n","        return body\n","\n","\n","class PushTImageEnv(PushTEnv):\n","    metadata = {\"render.modes\": [\"rgb_array\"], \"video.frames_per_second\": 10}\n","\n","    def __init__(self,\n","            legacy=False,\n","            block_cog=None,\n","            damping=None,\n","            render_size=96):\n","        super().__init__(\n","            legacy=legacy,\n","            block_cog=block_cog,\n","            damping=damping,\n","            render_size=render_size,\n","            render_action=False)\n","        ws = self.window_size\n","        self.observation_space = spaces.Dict({\n","            'image': spaces.Box(\n","                low=0,\n","                high=1,\n","                shape=(3,render_size,render_size),\n","                dtype=np.float32\n","            ),\n","            'agent_pos': spaces.Box(\n","                low=0,\n","                high=ws,\n","                shape=(2,),\n","                dtype=np.float32\n","            )        \n","        })\n","        self.render_cache = None\n","\n","    def _get_obs(self):\n","        img = super()._render_frame(mode='rgb_array')\n","\n","        agent_pos = np.array(self.agent.position)        \n","        img_obs = np.moveaxis(img.astype(np.float32) / 255, -1, 0)\n","        obs = {\n","            'image': img_obs,\n","            'agent_pos': agent_pos,\n","        }\n","\n","        # draw action\n","        if self.latest_action is not None:\n","            action = np.array(self.latest_action)\n","            coord = (action / 512 * 96).astype(np.int32)\n","            marker_size = int(8/96*self.render_size)\n","            thickness = int(1/96*self.render_size)\n","            cv2.drawMarker(img, coord,\n","                color=(255,0,0), markerType=cv2.MARKER_CROSS,\n","                markerSize=marker_size, thickness=thickness)\n","        self.render_cache = img\n","\n","        return obs\n","\n","    def render(self, mode):\n","        assert mode == 'rgb_array'\n","\n","        if self.render_cache is None:\n","            self._get_obs()\n","\n","        return self.render_cache\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### **Env Demo**\n","Standard Gym Env (0.21.0 API)"]},{"cell_type":"code","execution_count":93,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1702476299850,"user":{"displayName":"Felix Herrmann","userId":"07905804872157634150"},"user_tz":-60},"id":"OknH8Qfqrtc9","outputId":"42e7ef77-fcf5-4c20-b7fe-0903e0fcdccd"},"outputs":[{"name":"stdout","output_type":"stream","text":["obs['image'].shape: (3, 96, 96) float32, [0,1]\n","obs['agent_pos'].shape: (2,) float32, [0,512]\n","action.shape:  (2,) float32, [0,512]\n","obs['tee_mask'].shape: (512, 512) uint8, {0,1}\n","obs['goal_mask'].shape: (512, 512) uint8, {0,1}\n","obs['agent_mask'].shape: (512, 512) uint8, {0,1}\n"]}],"source":["# 0. create env object\n","env = PushTImageEnv()\n","\n","# 1. seed env for initial state.\n","# Seed 0-200 are used for the demonstration dataset.\n","env.seed(1000)\n","\n","# 2. must reset before use\n","obs, info = env.reset()\n","\n","# 3. 2D positional action space [0,512]\n","action = env.action_space.sample()\n","\n","# 4. Standard gym step method\n","obs, reward, terminated, truncated, info = env.step(action)\n","\n","# prints and explains each dimension of the observation and action vectors\n","with np.printoptions(precision=4, suppress=True, threshold=5):\n","    print(\"obs['image'].shape:\", obs['image'].shape, \"float32, [0,1]\")\n","    print(\"obs['agent_pos'].shape:\", obs['agent_pos'].shape, \"float32, [0,512]\")\n","    print(\"action.shape: \", action.shape, \"float32, [0,512]\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"vHepJOFBucwg"},"outputs":[],"source":["#@markdown ### **Dataset**\n","#@markdown\n","#@markdown Defines `PushTImageDataset` and helper functions\n","#@markdown\n","#@markdown The dataset class\n","#@markdown - Load data ((image, agent_pos), action) from a zarr storage\n","#@markdown - Normalizes each dimension of agent_pos and action to [-1,1]\n","#@markdown - Returns\n","#@markdown  - All possible segments with length `pred_horizon`\n","#@markdown  - Pads the beginning and the end of each episode with repetition\n","#@markdown  - key `image`: shape (obs_hoirzon, 3, 96, 96)\n","#@markdown  - key `agent_pos`: shape (obs_hoirzon, 2)\n","#@markdown  - key `action`: shape (pred_horizon, 2)\n","\n","def create_sample_indices(\n","        episode_ends:np.ndarray, sequence_length:int,\n","        pad_before: int=0, pad_after: int=0):\n","    indices = list()\n","    for i in range(len(episode_ends)):\n","        start_idx = 0\n","        if i > 0:\n","            start_idx = episode_ends[i-1]\n","        end_idx = episode_ends[i]\n","        episode_length = end_idx - start_idx\n","\n","        min_start = -pad_before\n","        max_start = episode_length - sequence_length + pad_after\n","\n","        # range stops one idx before end\n","        for idx in range(min_start, max_start+1):\n","            buffer_start_idx = max(idx, 0) + start_idx\n","            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx\n","            start_offset = buffer_start_idx - (idx+start_idx)\n","            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx\n","            sample_start_idx = 0 + start_offset\n","            sample_end_idx = sequence_length - end_offset\n","            indices.append([\n","                buffer_start_idx, buffer_end_idx,\n","                sample_start_idx, sample_end_idx])\n","    indices = np.array(indices)\n","    return indices\n","\n","\n","def sample_sequence(train_data, sequence_length,\n","                    buffer_start_idx, buffer_end_idx,\n","                    sample_start_idx, sample_end_idx):\n","    result = dict()\n","    for key, input_arr in train_data.items():\n","        sample = input_arr[buffer_start_idx:buffer_end_idx]\n","        data = sample\n","        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n","            data = np.zeros(\n","                shape=(sequence_length,) + input_arr.shape[1:],\n","                dtype=input_arr.dtype)\n","            if sample_start_idx > 0:\n","                data[:sample_start_idx] = sample[0]\n","            if sample_end_idx < sequence_length:\n","                data[sample_end_idx:] = sample[-1]\n","            data[sample_start_idx:sample_end_idx] = sample\n","        result[key] = data\n","    return result\n","\n","# normalize data\n","def get_data_stats(data):\n","    data = data.reshape(-1,data.shape[-1])\n","    stats = {\n","        'min': np.min(data, axis=0),\n","        'max': np.max(data, axis=0)\n","    }\n","    return stats\n","\n","def normalize_data(data, stats):\n","    # nomalize to [0,1]\n","    ndata = (data - stats['min']) / (stats['max'] - stats['min'])\n","    # normalize to [-1, 1]\n","    ndata = ndata * 2 - 1\n","    return ndata\n","\n","def unnormalize_data(ndata, stats):\n","    ndata = (ndata + 1) / 2\n","    data = ndata * (stats['max'] - stats['min']) + stats['min']\n","    return data\n","\n","# dataset\n","class PushTImageDataset(torch.utils.data.Dataset):\n","    def __init__(self,\n","                 dataset_path: str,\n","                 pred_horizon: int,\n","                 obs_horizon: int,\n","                 action_horizon: int):\n","\n","        # read from zarr dataset\n","        dataset_root = zarr.open(dataset_path, 'r')\n","\n","        # float32, [0,1], (N,96,96,3)\n","        train_image_data = dataset_root['data']['img'][:]\n","        train_image_data = np.moveaxis(train_image_data, -1,1)\n","        # (N,3,96,96)\n","\n","        # (N, D)\n","        train_data = {\n","            # first two dims of state vector are agent (i.e. gripper) locations\n","            'agent_pos': dataset_root['data']['state'][:,:2],\n","            'action': dataset_root['data']['action'][:]\n","        }\n","        episode_ends = dataset_root['meta']['episode_ends'][:]\n","\n","        # compute start and end of each state-action sequence\n","        # also handles padding\n","        indices = create_sample_indices(\n","            episode_ends=episode_ends,\n","            sequence_length=pred_horizon,\n","            pad_before=obs_horizon-1,\n","            pad_after=action_horizon-1)\n","\n","        # compute statistics and normalized data to [-1,1]\n","        stats = dict()\n","        normalized_train_data = dict()\n","        for key, data in train_data.items():\n","            stats[key] = get_data_stats(data)\n","            normalized_train_data[key] = normalize_data(data, stats[key])\n","\n","        # images are already normalized\n","        normalized_train_data['image'] = train_image_data\n","\n","        self.indices = indices\n","        self.stats = stats\n","        self.normalized_train_data = normalized_train_data\n","        self.pred_horizon = pred_horizon\n","        self.action_horizon = action_horizon\n","        self.obs_horizon = obs_horizon\n","\n","    def __len__(self):\n","        return len(self.indices)\n","\n","    def __getitem__(self, idx):\n","        # get the start/end indices for this datapoint\n","        buffer_start_idx, buffer_end_idx, \\\n","            sample_start_idx, sample_end_idx = self.indices[idx]\n","\n","        # get nomralized data using these indices\n","        nsample = sample_sequence(\n","            train_data=self.normalized_train_data,\n","            sequence_length=self.pred_horizon,\n","            buffer_start_idx=buffer_start_idx,\n","            buffer_end_idx=buffer_end_idx,\n","            sample_start_idx=sample_start_idx,\n","            sample_end_idx=sample_end_idx\n","        )\n","\n","        # discard unused observations\n","        nsample['image'] = nsample['image'][:self.obs_horizon,:]\n","        nsample['agent_pos'] = nsample['agent_pos'][:self.obs_horizon,:]\n","        return nsample\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10369,"status":"ok","timestamp":1698386278807,"user":{"displayName":"Chi Cheng","userId":"13145723388682673807"},"user_tz":420},"id":"9ZiHF3lzvB6k","outputId":"2c35318c-46a0-4f21-cf72-7d6bd5728072"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1KY1InLurpMvJDRb14L9NlXT_fEsCvVUq&confirm=t\n","To: /content/pusht_cchi_v7_replay.zarr.zip\n","100%|██████████| 31.1M/31.1M [00:00<00:00, 149MB/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"name":"stdout","output_type":"stream","text":["batch['image'].shape: torch.Size([64, 2, 3, 96, 96])\n","batch['agent_pos'].shape: torch.Size([64, 2, 2])\n","batch['action'].shape torch.Size([64, 16, 2])\n"]}],"source":["#@markdown ### **Dataset Demo**\n","\n","# download demonstration data from Google Drive\n","dataset_path = \"pusht_cchi_v7_replay.zarr.zip\"\n","if not os.path.isfile(dataset_path):\n","    id = \"1KY1InLurpMvJDRb14L9NlXT_fEsCvVUq&confirm=t\"\n","    gdown.download(id=id, output=dataset_path, quiet=False)\n","\n","# parameters\n","pred_horizon = 16\n","obs_horizon = 2\n","action_horizon = 8\n","#|o|o|                             observations: 2\n","#| |a|a|a|a|a|a|a|a|               actions executed: 8\n","#|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n","\n","# create dataset from file\n","dataset = PushTImageDataset(\n","    dataset_path=dataset_path,\n","    pred_horizon=pred_horizon,\n","    obs_horizon=obs_horizon,\n","    action_horizon=action_horizon\n",")\n","# save training data statistics (min, max) for each dim\n","stats = dataset.stats\n","\n","# create dataloader\n","dataloader = torch.utils.data.DataLoader(\n","    dataset,\n","    batch_size=64,\n","    num_workers=4,\n","    shuffle=True,\n","    # accelerate cpu-gpu transfer\n","    pin_memory=True,\n","    # don't kill worker process afte each epoch\n","    persistent_workers=True\n",")\n","\n","# visualize data in batch\n","batch = next(iter(dataloader))\n","print(\"batch['image'].shape:\", batch['image'].shape)\n","print(\"batch['agent_pos'].shape:\", batch['agent_pos'].shape)\n","print(\"batch['action'].shape\", batch['action'].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"X-XRB_g3vsgf"},"outputs":[],"source":["#@markdown ### **Network**\n","#@markdown\n","#@markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n","#@markdown as the noies prediction network\n","#@markdown\n","#@markdown Components\n","#@markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n","#@markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n","#@markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n","#@markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n","#@markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n","#@markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection.\n","#@markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n","\n","class SinusoidalPosEmb(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.dim = dim\n","\n","    def forward(self, x):\n","        device = x.device\n","        half_dim = self.dim // 2\n","        emb = math.log(10000) / (half_dim - 1)\n","        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n","        emb = x[:, None] * emb[None, :]\n","        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n","        return emb\n","\n","\n","class Downsample1d(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class Upsample1d(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","\n","class Conv1dBlock(nn.Module):\n","    '''\n","        Conv1d --> GroupNorm --> Mish\n","    '''\n","\n","    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n","        super().__init__()\n","\n","        self.block = nn.Sequential(\n","            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n","            nn.GroupNorm(n_groups, out_channels),\n","            nn.Mish(),\n","        )\n","\n","    def forward(self, x):\n","        return self.block(x)\n","\n","\n","class ConditionalResidualBlock1D(nn.Module):\n","    def __init__(self,\n","            in_channels,\n","            out_channels,\n","            cond_dim,\n","            kernel_size=3,\n","            n_groups=8):\n","        super().__init__()\n","\n","        self.blocks = nn.ModuleList([\n","            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n","            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n","        ])\n","\n","        # FiLM modulation https://arxiv.org/abs/1709.07871\n","        # predicts per-channel scale and bias\n","        cond_channels = out_channels * 2\n","        self.out_channels = out_channels\n","        self.cond_encoder = nn.Sequential(\n","            nn.Mish(),\n","            nn.Linear(cond_dim, cond_channels),\n","            nn.Unflatten(-1, (-1, 1))\n","        )\n","\n","        # make sure dimensions compatible\n","        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n","            if in_channels != out_channels else nn.Identity()\n","\n","    def forward(self, x, cond):\n","        '''\n","            x : [ batch_size x in_channels x horizon ]\n","            cond : [ batch_size x cond_dim]\n","\n","            returns:\n","            out : [ batch_size x out_channels x horizon ]\n","        '''\n","        out = self.blocks[0](x)\n","        embed = self.cond_encoder(cond)\n","\n","        embed = embed.reshape(\n","            embed.shape[0], 2, self.out_channels, 1)\n","        scale = embed[:,0,...]\n","        bias = embed[:,1,...]\n","        out = scale * out + bias\n","\n","        out = self.blocks[1](out)\n","        out = out + self.residual_conv(x)\n","        return out\n","\n","\n","class ConditionalUnet1D(nn.Module):\n","    def __init__(self,\n","        input_dim,\n","        global_cond_dim,\n","        diffusion_step_embed_dim=256,\n","        down_dims=[256,512,1024],\n","        kernel_size=5,\n","        n_groups=8\n","        ):\n","        \"\"\"\n","        input_dim: Dim of actions.\n","        global_cond_dim: Dim of global conditioning applied with FiLM\n","          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n","        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n","        down_dims: Channel size for each UNet level.\n","          The length of this array determines numebr of levels.\n","        kernel_size: Conv kernel size\n","        n_groups: Number of groups for GroupNorm\n","        \"\"\"\n","\n","        super().__init__()\n","        all_dims = [input_dim] + list(down_dims)\n","        start_dim = down_dims[0]\n","\n","        dsed = diffusion_step_embed_dim\n","        diffusion_step_encoder = nn.Sequential(\n","            SinusoidalPosEmb(dsed),\n","            nn.Linear(dsed, dsed * 4),\n","            nn.Mish(),\n","            nn.Linear(dsed * 4, dsed),\n","        )\n","        cond_dim = dsed + global_cond_dim\n","\n","        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n","        mid_dim = all_dims[-1]\n","        self.mid_modules = nn.ModuleList([\n","            ConditionalResidualBlock1D(\n","                mid_dim, mid_dim, cond_dim=cond_dim,\n","                kernel_size=kernel_size, n_groups=n_groups\n","            ),\n","            ConditionalResidualBlock1D(\n","                mid_dim, mid_dim, cond_dim=cond_dim,\n","                kernel_size=kernel_size, n_groups=n_groups\n","            ),\n","        ])\n","\n","        down_modules = nn.ModuleList([])\n","        for ind, (dim_in, dim_out) in enumerate(in_out):\n","            is_last = ind >= (len(in_out) - 1)\n","            down_modules.append(nn.ModuleList([\n","                ConditionalResidualBlock1D(\n","                    dim_in, dim_out, cond_dim=cond_dim,\n","                    kernel_size=kernel_size, n_groups=n_groups),\n","                ConditionalResidualBlock1D(\n","                    dim_out, dim_out, cond_dim=cond_dim,\n","                    kernel_size=kernel_size, n_groups=n_groups),\n","                Downsample1d(dim_out) if not is_last else nn.Identity()\n","            ]))\n","\n","        up_modules = nn.ModuleList([])\n","        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n","            is_last = ind >= (len(in_out) - 1)\n","            up_modules.append(nn.ModuleList([\n","                ConditionalResidualBlock1D(\n","                    dim_out*2, dim_in, cond_dim=cond_dim,\n","                    kernel_size=kernel_size, n_groups=n_groups),\n","                ConditionalResidualBlock1D(\n","                    dim_in, dim_in, cond_dim=cond_dim,\n","                    kernel_size=kernel_size, n_groups=n_groups),\n","                Upsample1d(dim_in) if not is_last else nn.Identity()\n","            ]))\n","\n","        final_conv = nn.Sequential(\n","            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n","            nn.Conv1d(start_dim, input_dim, 1),\n","        )\n","\n","        self.diffusion_step_encoder = diffusion_step_encoder\n","        self.up_modules = up_modules\n","        self.down_modules = down_modules\n","        self.final_conv = final_conv\n","\n","        print(\"number of parameters: {:e}\".format(\n","            sum(p.numel() for p in self.parameters()))\n","        )\n","\n","    def forward(self,\n","            sample: torch.Tensor,\n","            timestep: Union[torch.Tensor, float, int],\n","            global_cond=None):\n","        \"\"\"\n","        x: (B,T,input_dim)\n","        timestep: (B,) or int, diffusion step\n","        global_cond: (B,global_cond_dim)\n","        output: (B,T,input_dim)\n","        \"\"\"\n","        # (B,T,C)\n","        sample = sample.moveaxis(-1,-2)\n","        # (B,C,T)\n","\n","        # 1. time\n","        timesteps = timestep\n","        if not torch.is_tensor(timesteps):\n","            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n","            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n","        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n","            timesteps = timesteps[None].to(sample.device)\n","        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n","        timesteps = timesteps.expand(sample.shape[0])\n","\n","        global_feature = self.diffusion_step_encoder(timesteps)\n","\n","        if global_cond is not None:\n","            global_feature = torch.cat([\n","                global_feature, global_cond\n","            ], axis=-1)\n","\n","        x = sample\n","        h = []\n","        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n","            x = resnet(x, global_feature)\n","            x = resnet2(x, global_feature)\n","            h.append(x)\n","            x = downsample(x)\n","\n","        for mid_module in self.mid_modules:\n","            x = mid_module(x, global_feature)\n","\n","        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n","            x = torch.cat((x, h.pop()), dim=1)\n","            x = resnet(x, global_feature)\n","            x = resnet2(x, global_feature)\n","            x = upsample(x)\n","\n","        x = self.final_conv(x)\n","\n","        # (B,C,T)\n","        x = x.moveaxis(-1,-2)\n","        # (B,T,C)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yXq4r744aMh1"},"outputs":[],"source":["#@markdown ### **Vision Encoder**\n","#@markdown\n","#@markdown Defines helper functions:\n","#@markdown - `get_resnet` to initialize standard ResNet vision encoder\n","#@markdown - `replace_bn_with_gn` to replace all BatchNorm layers with GroupNorm\n","\n","def get_resnet(name:str, weights=None, **kwargs) -> nn.Module:\n","    \"\"\"\n","    name: resnet18, resnet34, resnet50\n","    weights: \"IMAGENET1K_V1\", None\n","    \"\"\"\n","    # Use standard ResNet implementation from torchvision\n","    func = getattr(torchvision.models, name)\n","    resnet = func(weights=weights, **kwargs)\n","\n","    # remove the final fully connected layer\n","    # for resnet18, the output dim should be 512\n","    resnet.fc = torch.nn.Identity()\n","    return resnet\n","\n","\n","def replace_submodules(\n","        root_module: nn.Module,\n","        predicate: Callable[[nn.Module], bool],\n","        func: Callable[[nn.Module], nn.Module]) -> nn.Module:\n","    \"\"\"\n","    Replace all submodules selected by the predicate with\n","    the output of func.\n","\n","    predicate: Return true if the module is to be replaced.\n","    func: Return new module to use.\n","    \"\"\"\n","    if predicate(root_module):\n","        return func(root_module)\n","\n","    bn_list = [k.split('.') for k, m\n","        in root_module.named_modules(remove_duplicate=True)\n","        if predicate(m)]\n","    for *parent, k in bn_list:\n","        parent_module = root_module\n","        if len(parent) > 0:\n","            parent_module = root_module.get_submodule('.'.join(parent))\n","        if isinstance(parent_module, nn.Sequential):\n","            src_module = parent_module[int(k)]\n","        else:\n","            src_module = getattr(parent_module, k)\n","        tgt_module = func(src_module)\n","        if isinstance(parent_module, nn.Sequential):\n","            parent_module[int(k)] = tgt_module\n","        else:\n","            setattr(parent_module, k, tgt_module)\n","    # verify that all modules are replaced\n","    bn_list = [k.split('.') for k, m\n","        in root_module.named_modules(remove_duplicate=True)\n","        if predicate(m)]\n","    assert len(bn_list) == 0\n","    return root_module\n","\n","def replace_bn_with_gn(\n","    root_module: nn.Module,\n","    features_per_group: int=16) -> nn.Module:\n","    \"\"\"\n","    Relace all BatchNorm layers with GroupNorm.\n","    \"\"\"\n","    replace_submodules(\n","        root_module=root_module,\n","        predicate=lambda x: isinstance(x, nn.BatchNorm2d),\n","        func=lambda x: nn.GroupNorm(\n","            num_groups=x.num_features//features_per_group,\n","            num_channels=x.num_features)\n","    )\n","    return root_module\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1288,"status":"ok","timestamp":1698386287483,"user":{"displayName":"Chi Cheng","userId":"13145723388682673807"},"user_tz":420},"id":"4APZkqh336-M","outputId":"07944c51-a8a6-45f0-cc02-f1e33ed45968"},"outputs":[{"name":"stdout","output_type":"stream","text":["number of parameters: 7.994727e+07\n"]}],"source":["#@markdown ### **Network Demo**\n","\n","# construct ResNet18 encoder\n","# if you have multiple camera views, use seperate encoder weights for each view.\n","vision_encoder = get_resnet('resnet18')\n","\n","# IMPORTANT!\n","# replace all BatchNorm with GroupNorm to work with EMA\n","# performance will tank if you forget to do this!\n","vision_encoder = replace_bn_with_gn(vision_encoder)\n","\n","# ResNet18 has output dim of 512\n","vision_feature_dim = 512\n","# agent_pos is 2 dimensional\n","lowdim_obs_dim = 2\n","# observation feature has 514 dims in total per step\n","obs_dim = vision_feature_dim + lowdim_obs_dim\n","action_dim = 2\n","\n","# create network object\n","noise_pred_net = ConditionalUnet1D(\n","    input_dim=action_dim,\n","    global_cond_dim=obs_dim*obs_horizon\n",")\n","\n","# the final arch has 2 parts\n","nets = nn.ModuleDict({\n","    'vision_encoder': vision_encoder,\n","    'noise_pred_net': noise_pred_net\n","})\n","\n","# demo\n","with torch.no_grad():\n","    # example inputs\n","    image = torch.zeros((1, obs_horizon,3,96,96))\n","    agent_pos = torch.zeros((1, obs_horizon, 2))\n","    # vision encoder\n","    image_features = nets['vision_encoder'](\n","        image.flatten(end_dim=1))\n","    # (2,512)\n","    image_features = image_features.reshape(*image.shape[:2],-1)\n","    # (1,2,512)\n","    obs = torch.cat([image_features, agent_pos],dim=-1)\n","    # (1,2,514)\n","\n","    noised_action = torch.randn((1, pred_horizon, action_dim))\n","    diffusion_iter = torch.zeros((1,))\n","\n","    # the noise prediction network\n","    # takes noisy action, diffusion iteration and observation as input\n","    # predicts the noise added to action\n","    noise = nets['noise_pred_net'](\n","        sample=noised_action,\n","        timestep=diffusion_iter,\n","        global_cond=obs.flatten(start_dim=1))\n","\n","    # illustration of removing noise\n","    # the actual noise removal is performed by NoiseScheduler\n","    # and is dependent on the diffusion noise schedule\n","    denoised_action = noised_action - noise\n","\n","# for this demo, we use DDPMScheduler with 100 diffusion iterations\n","num_diffusion_iters = 100\n","noise_scheduler = DDPMScheduler(\n","    num_train_timesteps=num_diffusion_iters,\n","    # the choise of beta schedule has big impact on performance\n","    # we found squared cosine works the best\n","    beta_schedule='squaredcos_cap_v2',\n","    # clip output to [-1,1] to improve stability\n","    clip_sample=True,\n","    # our network predicts noise (instead of denoised action)\n","    prediction_type='epsilon'\n",")\n","\n","# device transfer\n","device = torch.device('cuda')\n","_ = nets.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":445,"referenced_widgets":["200f515cb4214f4cb520d8f8bc1f36a0","1d78a8564d4945548183e41c7f949e1a","f0989fb473e74cb8ac82b73b1cd9de6c","e14153bfbaf84c79ba274c32abcc775e","bf0322956b4c41e5aaecae47c2724374","9757b998754f4e8398cf336c6dcfa975","740cc90c8dc241a28a497770bae1e0a7","286953f56dbe49ac852b3dac4d34d4eb","c50e8b171f4a4e41baad5bf8a79da122","6f49b866d0f9476995b6f87aabc04cdd","a1d6550c4abe4002a7eed9b0d30be92d","e56398a656924094925ed6da9639af90","0d612e88990e4df280014354e024285a","68c4cf61318943cb88bc155dfe8c6b89","66be6ae60f464dd6b1fe82740b45ae0d","8c88a1a25d79443c83db625da7a7ecb8","0130f6e35418428aad81460152ce3eb6","78eed74360f841ed9a89db3afeaa8577","2ab3d498e2e347f38c12e071fd0ca9d4","2ddab6a6b0bc420f9c7599805a69e0d9","db4cbd2f04ad403fb04a01a9de604d02","25e084e4cb0549bea3269298627ade20"]},"executionInfo":{"elapsed":4597,"status":"error","timestamp":1677175596149,"user":{"displayName":"Chi Cheng","userId":"13145723388682673807"},"user_tz":300},"id":"93E9RdnR4D8v","outputId":"697325c2-aa98-419a-f21c-4a2aec777d5f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"200f515cb4214f4cb520d8f8bc1f36a0","version_major":2,"version_minor":0},"text/plain":["Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e56398a656924094925ed6da9639af90","version_major":2,"version_minor":0},"text/plain":["Batch:   0%|          | 0/379 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-484363cd16ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;31m# update Exponential Moving Average of the model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_pred_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;31m# logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/diffusers/training_utils.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, new_model)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mema_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mema_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mema_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0mema_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mema_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#@markdown ### **Training**\n","#@markdown\n","#@markdown Takes about 2.5 hours. If you don't want to wait, skip to the next cell\n","#@markdown to load pre-trained weights\n","\n","num_epochs = 100\n","\n","# Exponential Moving Average\n","# accelerates training and improves stability\n","# holds a copy of the model weights\n","ema = EMAModel(\n","    model=nets,\n","    power=0.75)\n","\n","# Standard ADAM optimizer\n","# Note that EMA parametesr are not optimized\n","optimizer = torch.optim.AdamW(\n","    params=nets.parameters(),\n","    lr=1e-4, weight_decay=1e-6)\n","\n","# Cosine LR schedule with linear warmup\n","lr_scheduler = get_scheduler(\n","    name='cosine',\n","    optimizer=optimizer,\n","    num_warmup_steps=500,\n","    num_training_steps=len(dataloader) * num_epochs\n",")\n","\n","with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n","    # epoch loop\n","    for epoch_idx in tglobal:\n","        epoch_loss = list()\n","        # batch loop\n","        with tqdm(dataloader, desc='Batch', leave=False) as tepoch:\n","            for nbatch in tepoch:\n","                # data normalized in dataset\n","                # device transfer\n","                nimage = nbatch['image'][:,:obs_horizon].to(device)\n","                nagent_pos = nbatch['agent_pos'][:,:obs_horizon].to(device)\n","                naction = nbatch['action'].to(device)\n","                B = nagent_pos.shape[0]\n","\n","                # encoder vision features\n","                image_features = nets['vision_encoder'](\n","                    nimage.flatten(end_dim=1))\n","                image_features = image_features.reshape(\n","                    *nimage.shape[:2],-1)\n","                # (B,obs_horizon,D)\n","\n","                # concatenate vision feature and low-dim obs\n","                obs_features = torch.cat([image_features, nagent_pos], dim=-1)\n","                obs_cond = obs_features.flatten(start_dim=1)\n","                # (B, obs_horizon * obs_dim)\n","\n","                # sample noise to add to actions\n","                noise = torch.randn(naction.shape, device=device)\n","\n","                # sample a diffusion iteration for each data point\n","                timesteps = torch.randint(\n","                    0, noise_scheduler.config.num_train_timesteps,\n","                    (B,), device=device\n","                ).long()\n","\n","                # add noise to the clean images according to the noise magnitude at each diffusion iteration\n","                # (this is the forward diffusion process)\n","                noisy_actions = noise_scheduler.add_noise(\n","                    naction, noise, timesteps)\n","\n","                # predict the noise residual\n","                noise_pred = noise_pred_net(\n","                    noisy_actions, timesteps, global_cond=obs_cond)\n","\n","                # L2 loss\n","                loss = nn.functional.mse_loss(noise_pred, noise)\n","\n","                # optimize\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                # step lr scheduler every batch\n","                # this is different from standard pytorch behavior\n","                lr_scheduler.step()\n","\n","                # update Exponential Moving Average of the model weights\n","                ema.step(nets)\n","\n","                # logging\n","                loss_cpu = loss.item()\n","                epoch_loss.append(loss_cpu)\n","                tepoch.set_postfix(loss=loss_cpu)\n","        tglobal.set_postfix(loss=np.mean(epoch_loss))\n","\n","# Weights of the EMA model\n","# is used for inference\n","ema_nets = ema.averaged_model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11736,"status":"ok","timestamp":1698386311747,"user":{"displayName":"Chi Cheng","userId":"13145723388682673807"},"user_tz":420},"id":"6F3hUbIuxGdO","outputId":"6dec0815-08b6-41ed-d8ca-7a991e1b34d4"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1XKpfNSlwYMGaF5CncoFaLKCDTWoLAHf1&confirm=t\n","To: /content/pusht_vision_100ep.ckpt\n","100%|██████████| 365M/365M [00:09<00:00, 37.6MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Pretrained weights loaded.\n"]}],"source":["#@markdown ### **Loading Pretrained Checkpoint**\n","#@markdown Set `load_pretrained = True` to load pretrained weights.\n","\n","load_pretrained = False\n","if load_pretrained:\n","  ckpt_path = \"pusht_vision_100ep.ckpt\"\n","  if not os.path.isfile(ckpt_path):\n","      id = \"1XKpfNSlwYMGaF5CncoFaLKCDTWoLAHf1&confirm=t\"\n","      gdown.download(id=id, output=ckpt_path, quiet=False)\n","\n","  state_dict = torch.load(ckpt_path, map_location='cuda')\n","  ema_nets = nets\n","  ema_nets.load_state_dict(state_dict)\n","  print('Pretrained weights loaded.')\n","else:\n","  print(\"Skipped pretrained weight loading.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":348,"referenced_widgets":["8f01f42ba7f34f46b4e8dbb22d5b378f","a7cf15c2c8ec4a38abd000b2e6231f31","10ad2e4461974eaa929e842b361f0693","14d40ededec3494e925acc96100d94a0","89d8a6075ad8428f99e5152263ec4336","4bc0d708648a47ec9cdde2676e87afd8","daa5cab05be1455da143e7734925ca8e","dfefc8a407dc4e5d82641a20dce9788e","7510c715ad0c4828965135ec78be435a","8a9c6514fb4b4a0d8daa0b61fc7ad373","96913dede40044e7ae4efa5aaf68fb38"]},"executionInfo":{"elapsed":40301,"status":"ok","timestamp":1698386355101,"user":{"displayName":"Chi Cheng","userId":"13145723388682673807"},"user_tz":420},"id":"OyLjlNQk5nr9","outputId":"07e25033-e5c7-45c0-b141-128ab6c6e76b"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f01f42ba7f34f46b4e8dbb22d5b378f","version_major":2,"version_minor":0},"text/plain":["Eval PushTImageEnv:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Score:  0.9873456160612877\n"]},{"data":{"text/html":["<video controls  width=\"256\"  height=\"256\">\n"," <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n"," Your browser does not support the video tag.\n"," </video>"],"text/plain":["<IPython.core.display.Video object>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["#@markdown ### **Inference**\n","\n","# limit enviornment interaction to 200 steps before termination\n","max_steps = 200\n","env = PushTImageEnv()\n","# use a seed >200 to avoid initial states seen in the training dataset\n","env.seed(100000)\n","\n","# get first observation\n","obs, info = env.reset()\n","\n","# keep a queue of last 2 steps of observations\n","obs_deque = collections.deque(\n","    [obs] * obs_horizon, maxlen=obs_horizon)\n","# save visualization and rewards\n","imgs = [env.render(mode='rgb_array')]\n","rewards = list()\n","done = False\n","step_idx = 0\n","\n","with tqdm(total=max_steps, desc=\"Eval PushTImageEnv\") as pbar:\n","    while not done:\n","        B = 1\n","        # stack the last obs_horizon number of observations\n","        images = np.stack([x['image'] for x in obs_deque])\n","        agent_poses = np.stack([x['agent_pos'] for x in obs_deque])\n","\n","        # normalize observation\n","        nagent_poses = normalize_data(agent_poses, stats=stats['agent_pos'])\n","        # images are already normalized to [0,1]\n","        nimages = images\n","\n","        # device transfer\n","        nimages = torch.from_numpy(nimages).to(device, dtype=torch.float32)\n","        # (2,3,96,96)\n","        nagent_poses = torch.from_numpy(nagent_poses).to(device, dtype=torch.float32)\n","        # (2,2)\n","\n","        # infer action\n","        with torch.no_grad():\n","            # get image features\n","            image_features = ema_nets['vision_encoder'](nimages)\n","            # (2,512)\n","\n","            # concat with low-dim observations\n","            obs_features = torch.cat([image_features, nagent_poses], dim=-1)\n","\n","            # reshape observation to (B,obs_horizon*obs_dim)\n","            obs_cond = obs_features.unsqueeze(0).flatten(start_dim=1)\n","\n","            # initialize action from Guassian noise\n","            noisy_action = torch.randn(\n","                (B, pred_horizon, action_dim), device=device)\n","            naction = noisy_action\n","\n","            # init scheduler\n","            noise_scheduler.set_timesteps(num_diffusion_iters)\n","\n","            for k in noise_scheduler.timesteps:\n","                # predict noise\n","                noise_pred = ema_nets['noise_pred_net'](\n","                    sample=naction,\n","                    timestep=k,\n","                    global_cond=obs_cond\n","                )\n","\n","                # inverse diffusion step (remove noise)\n","                naction = noise_scheduler.step(\n","                    model_output=noise_pred,\n","                    timestep=k,\n","                    sample=naction\n","                ).prev_sample\n","\n","        # unnormalize action\n","        naction = naction.detach().to('cpu').numpy()\n","        # (B, pred_horizon, action_dim)\n","        naction = naction[0]\n","        action_pred = unnormalize_data(naction, stats=stats['action'])\n","\n","        # only take action_horizon number of actions\n","        start = obs_horizon - 1\n","        end = start + action_horizon\n","        action = action_pred[start:end,:]\n","        # (action_horizon, action_dim)\n","\n","        # execute action_horizon number of steps\n","        # without replanning\n","        for i in range(len(action)):\n","            # stepping env\n","            obs, reward, done, _, info = env.step(action[i])\n","            # save observations\n","            obs_deque.append(obs)\n","            # and reward/vis\n","            rewards.append(reward)\n","            imgs.append(env.render(mode='rgb_array'))\n","\n","            # update progress bar\n","            step_idx += 1\n","            pbar.update(1)\n","            pbar.set_postfix(reward=reward)\n","            if step_idx > max_steps:\n","                done = True\n","            if done:\n","                break\n","\n","# print out the maximum target coverage\n","print('Score: ', max(rewards))\n","\n","# visualize\n","from IPython.display import Video\n","vwrite('vis.mp4', imgs)\n","Video('vis.mp4', embed=True, width=256, height=256)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"18GIHeOQ5DyjMN8iIRZL2EKZ0745NLIpg","timestamp":1702476076708}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0130f6e35418428aad81460152ce3eb6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b0ca1b2ad5842128c4a38b5d4439298":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56391f008fe74cdaa1ba1748154abf8f","placeholder":"​","style":"IPY_MODEL_5fa086a4e27b40c39f56bb259bdca964","value":""}},"0d612e88990e4df280014354e024285a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0130f6e35418428aad81460152ce3eb6","placeholder":"​","style":"IPY_MODEL_78eed74360f841ed9a89db3afeaa8577","value":"Batch:   4%"}},"10ad2e4461974eaa929e842b361f0693":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfefc8a407dc4e5d82641a20dce9788e","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7510c715ad0c4828965135ec78be435a","value":200}},"119814c5a6904193a5cca0a4fd67a47e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"14d40ededec3494e925acc96100d94a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a9c6514fb4b4a0d8daa0b61fc7ad373","placeholder":"​","style":"IPY_MODEL_96913dede40044e7ae4efa5aaf68fb38","value":" 201/? [00:39&lt;00:00,  5.76it/s, reward=0.632]"}},"18d2e12e4b224a22b1d1aa76c98a3ae4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d78a8564d4945548183e41c7f949e1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9757b998754f4e8398cf336c6dcfa975","placeholder":"​","style":"IPY_MODEL_740cc90c8dc241a28a497770bae1e0a7","value":"Epoch:   0%"}},"200f515cb4214f4cb520d8f8bc1f36a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1d78a8564d4945548183e41c7f949e1a","IPY_MODEL_f0989fb473e74cb8ac82b73b1cd9de6c","IPY_MODEL_e14153bfbaf84c79ba274c32abcc775e"],"layout":"IPY_MODEL_bf0322956b4c41e5aaecae47c2724374"}},"22c2c5ca95b94e1daa707147d098f8ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25e084e4cb0549bea3269298627ade20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"286953f56dbe49ac852b3dac4d34d4eb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ab3d498e2e347f38c12e071fd0ca9d4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ddab6a6b0bc420f9c7599805a69e0d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2ed040bf97024baca8cefaa0e35bed9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b0ca1b2ad5842128c4a38b5d4439298","IPY_MODEL_915df5cf55204847bb3ebe17417dd200","IPY_MODEL_795c32269ac54be5b832a58f21d7c8d3"],"layout":"IPY_MODEL_18d2e12e4b224a22b1d1aa76c98a3ae4"}},"4bc0d708648a47ec9cdde2676e87afd8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56391f008fe74cdaa1ba1748154abf8f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fa086a4e27b40c39f56bb259bdca964":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66be6ae60f464dd6b1fe82740b45ae0d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db4cbd2f04ad403fb04a01a9de604d02","placeholder":"​","style":"IPY_MODEL_25e084e4cb0549bea3269298627ade20","value":" 14/379 [00:04&lt;01:26,  4.24it/s, loss=0.00215]"}},"68c4cf61318943cb88bc155dfe8c6b89":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ab3d498e2e347f38c12e071fd0ca9d4","max":379,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ddab6a6b0bc420f9c7599805a69e0d9","value":14}},"6f49b866d0f9476995b6f87aabc04cdd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"740cc90c8dc241a28a497770bae1e0a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7510c715ad0c4828965135ec78be435a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"78eed74360f841ed9a89db3afeaa8577":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"795c32269ac54be5b832a58f21d7c8d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8c100153bd94663bfc83071c81eb7a4","placeholder":"​","style":"IPY_MODEL_22c2c5ca95b94e1daa707147d098f8ca","value":" 0/0 [00:00&lt;?, ?it/s]"}},"89d8a6075ad8428f99e5152263ec4336":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a9c6514fb4b4a0d8daa0b61fc7ad373":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c88a1a25d79443c83db625da7a7ecb8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f01f42ba7f34f46b4e8dbb22d5b378f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a7cf15c2c8ec4a38abd000b2e6231f31","IPY_MODEL_10ad2e4461974eaa929e842b361f0693","IPY_MODEL_14d40ededec3494e925acc96100d94a0"],"layout":"IPY_MODEL_89d8a6075ad8428f99e5152263ec4336"}},"915df5cf55204847bb3ebe17417dd200":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_119814c5a6904193a5cca0a4fd67a47e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_95b06f2fff024d3c8a3a447402074922","value":0}},"95b06f2fff024d3c8a3a447402074922":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"96913dede40044e7ae4efa5aaf68fb38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9757b998754f4e8398cf336c6dcfa975":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1d6550c4abe4002a7eed9b0d30be92d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7cf15c2c8ec4a38abd000b2e6231f31":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bc0d708648a47ec9cdde2676e87afd8","placeholder":"​","style":"IPY_MODEL_daa5cab05be1455da143e7734925ca8e","value":"Eval PushTImageEnv: "}},"bf0322956b4c41e5aaecae47c2724374":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c50e8b171f4a4e41baad5bf8a79da122":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c8c100153bd94663bfc83071c81eb7a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"daa5cab05be1455da143e7734925ca8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db4cbd2f04ad403fb04a01a9de604d02":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfefc8a407dc4e5d82641a20dce9788e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e14153bfbaf84c79ba274c32abcc775e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f49b866d0f9476995b6f87aabc04cdd","placeholder":"​","style":"IPY_MODEL_a1d6550c4abe4002a7eed9b0d30be92d","value":" 0/100 [00:04&lt;?, ?it/s]"}},"e56398a656924094925ed6da9639af90":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0d612e88990e4df280014354e024285a","IPY_MODEL_68c4cf61318943cb88bc155dfe8c6b89","IPY_MODEL_66be6ae60f464dd6b1fe82740b45ae0d"],"layout":"IPY_MODEL_8c88a1a25d79443c83db625da7a7ecb8"}},"f0989fb473e74cb8ac82b73b1cd9de6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_286953f56dbe49ac852b3dac4d34d4eb","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c50e8b171f4a4e41baad5bf8a79da122","value":0}}}}},"nbformat":4,"nbformat_minor":0}
