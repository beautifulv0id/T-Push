{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/miniforge3/envs/tshape/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.ops import FeaturePyramidNetwork\n",
    "from utils.clip import load_clip\n",
    "import einops\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils.layers import RelativeCrossAttentionModule\n",
    "from models.conditional_unet1d import ConditionalUnet1D\n",
    "from utils.position_encodings import SinusoidalPosEmb\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from utils.position_encodings import RotaryPositionEncoding, RotaryPositionEncoding3D, RotaryPositionEncoding2D\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PushTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, rgb, goal_mask, tee_mask, agent_mask, agent_pos, goal_pose):\n",
    "        self.rgb = rgb\n",
    "        self.target = goal_mask\n",
    "        self.tee_mask = tee_mask\n",
    "        self.agent_mask = agent_mask\n",
    "        self.agent_pos = agent_pos\n",
    "        self.goal_pose = goal_pose\n",
    "        self.positions = self._get_positions()\n",
    "\n",
    "    def _get_positions(self):\n",
    "        xy = torch.meshgrid([torch.arange(0, self.rgb.shape[2]), torch.arange(0, self.rgb.shape[3])])\n",
    "        xy = torch.stack(xy, dim=0).float()\n",
    "        return xy\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = {}\n",
    "        item['rgb'] = self.rgb[index]\n",
    "        item['goal_mask'] = self.target[index]\n",
    "        item['tee_mask'] = self.tee_mask[index]\n",
    "        item['agent_mask'] = self.agent_mask[index]\n",
    "        item['agent_pos'] = self.agent_pos[index]\n",
    "        item['goal_pose'] = self.goal_pose[index]\n",
    "        item['positions'] = self.positions\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb)\n",
    "\n",
    "\n",
    "def get_loader(data_dir='../data/'):\n",
    "    files = os.listdir(data_dir)\n",
    "    files = [f for f in files if f.endswith('.npz')]\n",
    "\n",
    "    rgb = []\n",
    "    goal_mask = []\n",
    "    tee_mask = []\n",
    "    agent_mask = []\n",
    "    agent_pos = []\n",
    "    goal_pose = []\n",
    "\n",
    "    for f in files:\n",
    "        el = np.load(data_dir + f)\n",
    "        rgb.append(el['image'])\n",
    "        goal_mask.append(el['goal_mask'])\n",
    "        tee_mask.append(el['tee_mask'])\n",
    "        agent_mask.append(el['agent_mask'])\n",
    "        agent_pos.append(el['agent_pos'])\n",
    "        goal_pose.append(el['goal_pose'])\n",
    "\n",
    "    rgb = np.stack(rgb, axis=0)\n",
    "    goal_mask = np.stack(goal_mask, axis=0)\n",
    "    tee_mask = np.stack(tee_mask, axis=0)\n",
    "    agent_mask = np.stack(agent_mask, axis=0)\n",
    "    agent_pos = np.stack(agent_pos, axis=0)\n",
    "    goal_pose = np.stack(goal_pose, axis=0)\n",
    "\n",
    "    rgb = torch.from_numpy(rgb).float()\n",
    "    goal_mask = torch.from_numpy(goal_mask).float()\n",
    "    tee_mask = torch.from_numpy(tee_mask).float()\n",
    "    agent_mask = torch.from_numpy(agent_mask).float()\n",
    "    agent_pos = torch.from_numpy(agent_pos).float()\n",
    "    goal_pose = torch.from_numpy(goal_pose).float()\n",
    "    \n",
    "    dataset = PushTDataset(rgb, goal_mask, tee_mask, agent_mask, agent_pos, goal_pose)\n",
    "\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item['rgb'].shape torch.Size([10, 3, 96, 96])\n",
      "item['goal_mask'].shape torch.Size([10, 96, 96])\n",
      "item['tee_mask'].shape torch.Size([10, 96, 96])\n",
      "item['agent_mask'].shape torch.Size([10, 96, 96])\n",
      "item['agent_pos'].shape torch.Size([10, 2])\n",
      "item['goal_pose'].shape torch.Size([10, 3])\n",
      "item['positions'].shape torch.Size([10, 2, 96, 96])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/miniforge3/envs/tshape/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_loader()\n",
    "\n",
    "item = next(iter(train_loader))\n",
    "\n",
    "print(\"item['rgb'].shape\", item['rgb'].shape)\n",
    "print(\"item['goal_mask'].shape\", item['goal_mask'].shape)\n",
    "print(\"item['tee_mask'].shape\", item['tee_mask'].shape)\n",
    "print(\"item['agent_mask'].shape\", item['agent_mask'].shape)\n",
    "print(\"item['agent_pos'].shape\", item['agent_pos'].shape)\n",
    "print(\"item['goal_pose'].shape\", item['goal_pose'].shape)\n",
    "print(\"item['positions'].shape\", item['positions'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_visual_features(model, normalize, linear, images):\n",
    "    images = normalize(images)\n",
    "    with torch.no_grad():\n",
    "        img_features = model(images)[\"res1\"]\n",
    "    hw = img_features.shape[-2:]\n",
    "\n",
    "    img_features = einops.rearrange(img_features, 'b c h w -> b (h w) c')\n",
    "    img_features = linear(img_features)\n",
    "    img_features = einops.rearrange(img_features, 'b (h w) c -> b c h w', h=hw[0], w=hw[1])\n",
    "\n",
    "    img_features = F.interpolate(img_features, size=images.shape[-2:], mode='bilinear', align_corners=False)\n",
    "    return img_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scene_features(vis_features, positions, query_embedding, query_position, rotary_embedder, re_cross_attn):\n",
    "    batch_size = vis_features.shape[0]\n",
    "    target_sequence_length = query_position.shape[1]\n",
    "    query = query_embedding.weight.unsqueeze(1).repeat(target_sequence_length, batch_size, 1)\n",
    "    query_rel_pe = rotary_embedder(query_position)\n",
    "\n",
    "    value = einops.rearrange(vis_features, 'b c h w -> (h w) b c')\n",
    "    value_pos = positions\n",
    "    value_pos = einops.rearrange(value_pos, 'b c h w -> b (w h) c')\n",
    "    value_rel_pe = rotary_embedder(value_pos)\n",
    "\n",
    "    scene_embedding = re_cross_attn(query=query, value=value, query_pos=query_rel_pe, value_pos=value_rel_pe)\n",
    "    return scene_embedding[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FILM(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, cond_channels, hidden_channels=128, diffusion_step_embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.cond_channels = cond_channels\n",
    "        dsed = diffusion_step_embed_dim\n",
    "\n",
    "        # self.input_encoder = nn.Linear(in_channels, hidden_channels)\n",
    "        # self.cond_encoder = nn.Linear(cond_channels + dsed, 2*hidden_channels)\n",
    "\n",
    "        self.hidden_input_encoder = nn.ModuleList([\n",
    "            nn.Linear(in_channels, hidden_channels),\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "        ]\n",
    "        )\n",
    "        self.hidden_cond_encoder = nn.ModuleList([\n",
    "            nn.Linear(cond_channels + dsed, 2*hidden_channels),\n",
    "            nn.Linear(cond_channels + dsed, 2*hidden_channels),\n",
    "            nn.Linear(cond_channels + dsed, 2*hidden_channels),\n",
    "        ]\n",
    "        )\n",
    "\n",
    "        self.out_proj = nn.Linear(hidden_channels, in_channels)\n",
    "        self.act = nn.ReLU()\n",
    "        self.diffusion_step_encoder = nn.Sequential(\n",
    "            SinusoidalPosEmb(dsed),\n",
    "            nn.Linear(dsed, dsed * 4),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(dsed * 4, dsed),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, sample, timestep, global_cond):\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "        timesteps = timestep.expand(sample.shape[0])\n",
    "\n",
    "        global_feature = self.diffusion_step_encoder(timesteps)\n",
    "\n",
    "        global_feature = torch.cat([\n",
    "            global_feature, global_cond\n",
    "        ], axis=-1)\n",
    "\n",
    "        out = sample\n",
    "        for input_encoder, cond_encoder in zip(self.hidden_input_encoder, self.hidden_cond_encoder):\n",
    "            out = self.act(input_encoder(out))\n",
    "            scale, bias = self.act(cond_encoder(global_feature)).chunk(2, dim=-1)\n",
    "            out = out * scale + bias\n",
    "\n",
    "\n",
    "        # out = self.act(self.input_encoder(sample))\n",
    "        # scale, bias = self.act(self.cond_encoder(global_feature)).chunk(2, dim=-1)\n",
    "        # out = out * scale + bias\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 2)\n",
    "t = torch.randn(32)\n",
    "global_cond = torch.randn(32, 60)\n",
    "film = FILM(2, 60)\n",
    "out = film(x, t, global_cond)\n",
    "print(\"out.shape\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 100/100 [00:01<00:00, 67.71it/s, loss=1.08]\n"
     ]
    }
   ],
   "source": [
    "from models.ema_model import EMAModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "\n",
    "num_epochs = 100\n",
    "embedding_dim = 60\n",
    "action_dim = 2\n",
    "device = torch.device('cuda')\n",
    "\n",
    "vis_backbone, normalize = load_clip()\n",
    "vis_out_proj = nn.Linear(64, embedding_dim).to(device)\n",
    "re_cross_attn = RelativeCrossAttentionModule(embedding_dim=embedding_dim, num_attn_heads=3, num_layers=2).to(device)\n",
    "query_embedding = nn.Embedding(1, embedding_dim).to(device)\n",
    "rotary_embedder = RotaryPositionEncoding2D(embedding_dim).to(device)\n",
    "\n",
    "noise_pred_net = FILM(\n",
    "    in_channels=action_dim,  \n",
    "    cond_channels=embedding_dim,\n",
    "    hidden_channels=128,\n",
    "    diffusion_step_embed_dim=256\n",
    ")\n",
    "\n",
    "nets = nn.ModuleDict({\n",
    "    'vision_encoder': vis_backbone,\n",
    "    'vision_proj': vis_out_proj,\n",
    "    'noise_pred_net': noise_pred_net,\n",
    "})\n",
    "\n",
    "\n",
    "ema = EMAModel(nets)\n",
    "\n",
    "_ = nets.to(device)\n",
    "\n",
    "num_diffusion_iters = 100\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    # the choise of beta schedule has big impact on performance\n",
    "    # we found squared cosine works the best\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    # clip output to [-1,1] to improve stability\n",
    "    clip_sample=True,\n",
    "    # our network predicts noise (instead of denoised action)\n",
    "    prediction_type='epsilon'\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=nets.parameters(),\n",
    "    lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(train_loader) * num_epochs\n",
    ")\n",
    "\n",
    "def normalize_input(x):\n",
    "    return x / 255.\n",
    "\n",
    "def unnormalize_input(x):\n",
    "    return x * 255.\n",
    "\n",
    "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
    "    losses = list()\n",
    "    for epoch_idx in tglobal:\n",
    "        epoch_loss = list()\n",
    "        # batch loop\n",
    "        with tqdm(train_loader, desc='Batch', leave=False) as tepoch:\n",
    "            for item in tepoch:\n",
    "                rgb = item['rgb'].to(device)\n",
    "                positions = item['positions'].to(device)\n",
    "                agent_pos = item['agent_pos'].to(device)\n",
    "                goal_pos = item['goal_pose'][..., :2].to(device)\n",
    "\n",
    "                B = rgb.shape[0]\n",
    "\n",
    "                rel_goal_pos = goal_pos - agent_pos\n",
    "                rel_goal_pos = normalize_input(rel_goal_pos)\n",
    "                noise = torch.randn(rel_goal_pos.shape).to(device)\n",
    "\n",
    "                rgb_features = compute_visual_features(nets[\"vision_encoder\"], normalize, nets[\"vision_proj\"], rgb)\n",
    "                rgb_features = einops.rearrange(rgb_features, 'b c h w -> (h w) b c')\n",
    "                positions = einops.rearrange(positions, 'b c h w -> b (w h) c')\n",
    "                query_pos = einops.rearrange(agent_pos, 'b c -> b 1 c')\n",
    "\n",
    "                target_sequence_length = 1\n",
    "                query = query_embedding.weight.unsqueeze(1).repeat(target_sequence_length, B, 1).to(device)\n",
    "                query_rel_pe = rotary_embedder(query_pos)\n",
    "\n",
    "                context_features = rgb_features\n",
    "                context_pos = rotary_embedder(positions)\n",
    "\n",
    "                scene_embedding = re_cross_attn(query=query, value=context_features, query_pos=query_rel_pe, value_pos=context_pos)\n",
    "                scene_embedding = scene_embedding[-1].squeeze(0)\n",
    "\n",
    "                timesteps = torch.randint(\n",
    "                    0, noise_scheduler.config.num_train_timesteps,\n",
    "                    (B,), device=device\n",
    "                        ).long()\n",
    "\n",
    "                diffusion_iter = torch.zeros((B,)).to(device)\n",
    "\n",
    "                noisy_actions = noise_scheduler.add_noise(\n",
    "                    rel_goal_pos, noise, timesteps)\n",
    "\n",
    "\n",
    "                noise_pred = nets[\"noise_pred_net\"](sample=rel_goal_pos, timestep=diffusion_iter, global_cond=scene_embedding)\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # step lr scheduler every batch\n",
    "                # this is different from standard pytorch behavior\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                # update Exponential Moving Average of the model weights\n",
    "                ema.step(nets)\n",
    "\n",
    "                # logging\n",
    "                loss_cpu = loss.item()\n",
    "                epoch_loss.append(loss_cpu)\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "        epoch_loss = np.mean(epoch_loss)\n",
    "        losses.append(epoch_loss)\n",
    "        tglobal.set_postfix(loss=epoch_loss)\n",
    "\n",
    "ema_nets = ema.averaged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:  tensor([[-244.5673,  255.0000]], device='cuda:0')\n",
      "rel_goal_pos:  tensor([[131.1688, -63.6274]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "item = next(iter(train_loader))\n",
    "rgb = item['rgb'][:1].to(device)\n",
    "positions = item['positions'][:1].to(device)\n",
    "agent_pos = item['agent_pos'][:1].to(device)\n",
    "goal_pos = item['goal_pose'][..., :2][:1].to(device)\n",
    "\n",
    "B = rgb.shape[0]\n",
    "\n",
    "rel_goal_pos = goal_pos - agent_pos\n",
    "\n",
    "rgb_features = compute_visual_features(nets[\"vision_encoder\"], normalize, nets[\"vision_proj\"], rgb)\n",
    "rgb_features = einops.rearrange(rgb_features, 'b c h w -> (h w) b c')\n",
    "positions = einops.rearrange(positions, 'b c h w -> b (w h) c')\n",
    "query_pos = einops.rearrange(agent_pos, 'b c -> b 1 c')\n",
    "\n",
    "target_sequence_length = 1\n",
    "query = query_embedding.weight.unsqueeze(1).repeat(target_sequence_length, B, 1).to(device)\n",
    "query_rel_pe = rotary_embedder(query_pos)\n",
    "\n",
    "context_features = rgb_features\n",
    "context_pos = rotary_embedder(positions)\n",
    "\n",
    "scene_embedding = re_cross_attn(query=query, value=context_features, query_pos=query_rel_pe, value_pos=context_pos)\n",
    "scene_embedding = scene_embedding[-1].squeeze(0)\n",
    "\n",
    "noisy_action = torch.randn(\n",
    "    (B, action_dim), device=device)\n",
    "naction = noisy_action\n",
    "\n",
    "noise_scheduler.set_timesteps(num_inference_steps=num_diffusion_iters, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for k in noise_scheduler.timesteps:\n",
    "        noise_pred = nets['noise_pred_net'](\n",
    "                            sample=naction,\n",
    "                            timestep=k,\n",
    "                            global_cond=scene_embedding\n",
    "                        )\n",
    "                        # inverse diffusion step (remove noise)\n",
    "        naction = noise_scheduler.step(\n",
    "            model_output=noise_pred,\n",
    "            timestep=k,\n",
    "            sample=naction\n",
    "        ).prev_sample\n",
    "\n",
    "naction = unnormalize_input(naction)\n",
    "print(\"action: \", naction)\n",
    "print(\"rel_goal_pos: \", rel_goal_pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tshape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
